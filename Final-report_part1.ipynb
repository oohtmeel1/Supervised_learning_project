{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Beginning of project:\n",
    "# This project took a data set from the Enron scandal of the 2000s and used it for sentiment analysis. \n",
    "# It was a fun learning experience. \n",
    "# I used regex to help clean the many random items from the dataset, \n",
    "# I used Vader sentiment to help me with the sentiment analysis itself, because I had to \n",
    "# Do it by hand\n",
    "# Vader uses a lexicon and rule based algorithm to perform this analysis\n",
    "# I then used Sklearn to perform SVm classification and test on my data\n",
    "# My goal was to explain what the overall sentiment of the company was. \n",
    "#https://www.cs.cmu.edu/~enron/\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my supervised learning project I attempted to perform sentiment analysis on a set of text data in order to gauge the overall happiness at a particular company. \n",
    "\n",
    "The data set I used came originally from the Federal Energy Regulatory Comission, and then was purchased by a researcher at MIT, and after some other events it became public and you can find it by following a link in my report and on my github. \n",
    "\n",
    "The data set is from the Enron Scandal of the early 2000s. Where that company was accused of fraud and for some reason the FERC decided to make the data set public after the investigation. So it became a benchmark for spam detection and email parsing, at least for a while.\n",
    "\n",
    " So I had to perform a lot of cleaning, to get rid of names, emails, and other random junk. \n",
    "\n",
    "I used a lot of regex to parse the data and as I worked with this set I realized I missed something and then had to go back. Re-analyze everything, and then try again. \n",
    "So EDA is a little different for this type of problem. It is not the usual Petal length or sepal width mathematical problem we are used to. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, a ton of libraries\n",
    "# Pandas the usual, along with numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "pd.options.display.max_colwidth = 200\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you want to see it. But it crashes the github repo you might also have to change some stuff\n",
    "#df = pd.read_csv(\"enron_mail_201505.tar.gz\",compression='gzip', error_bad_lines=False, encoding = 'latin1')\n",
    "#df.to_csv(\"New folder/newfile1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tons of Regex, tons of stuff that worked and maybe did not\n",
    "pattern = \"^\\w+:\\s+[\\w.+-]+@[\\w-]+\\.[\\w.-]+\"\n",
    "filt = df['Unnamed:0'].str.contains(pattern)\n",
    "df = df[~filt]\n",
    "df.to_csv(\"Filtering_step_1.csv\")\n",
    "df1=pd.read_csv('Filtering_step_1.csv')\n",
    "pattern = \"^\\w+:\\s+[\\w.+-]+.\\s+[\\w.+-]+.*?ENRON\"\n",
    "filt = df1['Unnamed: 0'].str.contains(pattern,na=False)\n",
    "df = df1[~filt]\n",
    "pattern = \"Subject:.*($|\\s)\"\n",
    "df1 = df[~filt]\n",
    "new_df = pd.DataFrame(columns=['Dates', 'Times', 'Messages'])\n",
    "# First regex\n",
    "dates = r\"Date:.*$\"\n",
    "# Filtering the times\n",
    "times = r\"^.*?(PDT).\"\n",
    "# Now this might work to filter out a bunch of stuff(Anything that does not have :)\n",
    "messages = r\"^(?!.*:).*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtdate = []\n",
    "if len(new_df['Dates']) == 0:\n",
    "    filt = df['Unnamed: 0'].str.contains(dates,na=False)\n",
    "    filtdate = filt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[filt]\n",
    "df3=df3.rename(columns={\"Unnamed: 0.1\": \"Message\",\"Unnamed: 0\": \"Day\", \"maildir/\": \"Date_and_time\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I have something to use to search a list of indexes\n",
    "searching=df3.index.values\n",
    "# made some variables\n",
    "i = searching[1]\n",
    "j = searching[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher =list(searching)\n",
    "searcher=searcher[1:]\n",
    "searcher=searcher[1:]\n",
    "searcher=np.array(searcher)\n",
    "i =searcher[0]\n",
    "j = searcher[1]\n",
    "filt = df1.contains(messages,na=False) \n",
    "filt = df1['Unnamed: 0'].str.contains(messages,na=False) \n",
    "df7 = df1[filt]\n",
    "selection=df7['Unnamed: 0'] #Select\n",
    "selection = pd.DataFrame(selection)\n",
    "selection=selection.rename(columns={\"Unnamed: 0\": \"Message\"}) # Rename cols\n",
    "df5 = df3.append(selection, ignore_index=False) # append to my df\n",
    "df5 = df5.sort_index() # sort index\n",
    "df5.head(10)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
